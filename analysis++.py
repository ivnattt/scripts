# -*- coding: utf-8 -*-
"""Analysis++

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AvMR3wQdrqOqXYM912F9iyF_ME7aBlW

For Summarization
"""


import nltk
from nltk.corpus import stopwords 
from nltk.tokenize import sent_tokenize 
import re

# nltk.download('stopwords')
#
# nltk.download('punkt')

def clean_text(text):
    # remove backslash-apostrophe 
    text = re.sub("\'", "", text) 
    # remove everything except alphabets 
    text = re.sub("[^a-zA-Z]"," ",text) 
    # remove whitespaces 
    text = ' '.join(text.split()) 
   
    text = text.lower() 
    return text

stop_words = set(stopwords.words('english'))

# function to remove stopwords
def remove_stopwords(text):
    no_stopword_text = [w for w in text if not w in stop_words]
    return ' '.join(no_stopword_text)

#data = "Fuzzy Logic In the real world, sometimes we face a condition where it is difficult to recognize whether the condition is true or not, their fuzzy logic gives relevant flexibility for reasoning that leads to inaccuracies and uncertainties of any condition.Fuzzy logic is a technique that represents and modifies uncertain information by measuring the degree to which the hypothesis is correct. Fuzzy logic is also used for reasoning about naturally uncertain concepts.Natural Language Processing  Natural language processing depicts the developing methods that assist in communicating with machines using human languages such as English.NLP is the processing of the human language by computer programs, examples include; spam detection by looking at the subject of a line or text of an email and checking if it is junk.NLP tasks are text translation, sentiment analysis, and speech recognition. NLP is used by Twitter to percolate terroristic language from their tweets, by amazon to interpret user reviews and enhance user experience"

data = "This will be short."

sentence = data
corpuso=clean_text(sentence)
corpuso=corpuso.split(' ')
corpuso=remove_stopwords(corpuso)
corpuso=corpuso.split(' ')

freqTable = dict() 
for word in corpuso: 
    word = word.lower() 
    if word in freqTable: 
        freqTable[word] += 1
    else: 
        freqTable[word] = 1

sentences = sent_tokenize(data) 
sentenceValue = dict() 
   
for sentence in sentences: 
    for word, freq in freqTable.items(): 
        if word in sentence.lower(): 
            if sentence in sentenceValue: 
                sentenceValue[sentence] += freq 
            else: 
                sentenceValue[sentence] = freq 
   
   
   
sumValues = 0
for sentence in sentenceValue: 
    sumValues += sentenceValue[sentence]

average = int(sumValues / len(sentenceValue)) 
   
# Storing sentences into our summary. 
summary = '' 

for sentence in sentences: 
    if (sentence in sentenceValue) and (sentenceValue[sentence] > (1.2 * average)): 
        summary += " " + sentence 
if (len(summary)==0):
  print(data)
else:
  print(summary + str(len(summary)))